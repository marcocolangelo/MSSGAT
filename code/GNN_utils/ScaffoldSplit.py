from collections import defaultdict
from itertools import compress

import torch
from rdkit import Chem
from rdkit.Chem.Scaffolds import MurckoScaffold
import random
import numpy as np

"""ScaffoldSplit.py
Contenuto: Implementa metodi per dividere i dati chimici basandosi su scaffold, una struttura chimica comune.
Funzioni Principali:
puoi scegliere uno dei metodi tra scaffold_split,random_scaffold_split e scaffold_randomized_spliting_xiong per dividere i dati in training, validation e test set."""


""" 
Generare scaffold molecolari, che sono parti della molecola costituite da anelli e atomi collegati tra loro.
Gli scaffold Murcko sono una rappresentazione strutturale delle molecole che evidenzia gli anelli 
e gli atomi colleganti tra di loro, escludendo i gruppi laterali."""
class ScaffoldGenerator(object):
    """
    Generate molecular scaffolds.

    Parameters
    ----------
    include_chirality : : bool, optional (default False)
        Include chirality in scaffolds.
    """
    def __init__(self, include_chirality=False):
        self.include_chirality = include_chirality

    def get_scaffold(self, mol):
        """
        Get Murcko scaffolds for molecules.

        Murcko scaffolds are described in DOI: 10.1021/jm9602928. They are
        essentially that part of the molecule consisting of rings and the
        linker atoms between them.

        Parameters
        ----------
        mols : array_like
            Molecules.
        """
        return MurckoScaffold.MurckoScaffoldSmiles(
            mol=mol, includeChirality=self.include_chirality)


def generate_scaffold(smiles, include_chirality=False):
    """Compute the Bemis-Murcko scaffold for a SMILES string."""
    mol = Chem.MolFromSmiles(smiles)
    engine = ScaffoldGenerator(include_chirality=include_chirality)
    scaffold = engine.get_scaffold(mol)
    return scaffold

"""
La funzione split viene utilizzata per selezionare scaffold molecolari casuali 
da un dizionario di scaffold e verificare che la suddivisione del dataset sia bilanciata. 
viene utilizzata per bilanciare la suddivisione del dataset durante il processo di training di modelli di machine learning. 
Specificamente, serve a selezionare un sottoinsieme di scaffold molecolari in modo che le classi minoritarie siano 
adeguatamente rappresentate nel dataset di training. La funzione assicura che il numero di molecole selezionate sia ottimale e bilanciato, 
basandosi su parametri come minor_count, minor_class, count e optimal_count. 
Questa suddivisione equilibrata è cruciale per migliorare le prestazioni del modello e prevenire il bias verso le classi maggioritarie.

Parametri:
scaffolds_dict: Dizionario con scaffold come chiavi e indici di molecole come valori.
    Lo scaffolds_dict è fondamentale per creare suddivisioni bilanciate del dataset basate sugli scaffold molecolari. Questo dizionario contiene gli scaffold come chiavi e le liste degli indici delle molecole che appartengono a ciascuno scaffold come valori. 
    Generato dalla funzione scaffold_randomized_spliting_xiong.
smiles_tasks_df: DataFrame contenente SMILES e task associati.
tasks: Lista di task.
weights: Pesi per bilanciare le classi.
sample_size: Numero di scaffold da selezionare.
random_seed: Seme per la randomizzazione.

Attributi:
minor_count: Conta il numero di molecole appartenenti alla classe minoritaria per ciascun scaffold.
minor_class: Identifica la classe minoritaria tra le classi nel dataset, la classe che ha il minor numero di esempi o istanze nel dataset probabilmente,qui espresso tramite pesi
count: Conta il numero totale di molecole per ciascun scaffold.
optimal_count: Il numero ideale di molecole da selezionare per bilanciare il dataset basato su una distribuzione target.
"""
def split(scaffolds_dict, smiles_tasks_df, tasks, weights, sample_size, random_seed=0):
    count = 0
    minor_count = 0
    minor_class = np.argmax(weights[0])  # weights are inverse of the ratio
    minor_ratio = 1 / weights[0][minor_class]
    optimal_count = 0.1 * len(smiles_tasks_df)
    while (count < optimal_count * 0.9 or count > optimal_count * 1.1) \
            or (minor_count < minor_ratio * optimal_count * 0.9 \
                or minor_count > minor_ratio * optimal_count * 1.1):
        random_seed += 1
        random.seed(random_seed)
        scaffold = random.sample(list(scaffolds_dict.keys()), sample_size)
        count = sum([len(scaffolds_dict[scaffold]) for scaffold in scaffold])
        index = [index for scaffold in scaffold for index in scaffolds_dict[scaffold]]
        minor_count = len(smiles_tasks_df.iloc[index, :][smiles_tasks_df[tasks[0]] == minor_class])
    #     print(random)
    return scaffold, index
"""Nel contesto della funzione `split` definita in `ScaffoldSplit.py`, 
l'output `scaffold` rappresenta un sottoinsieme di scaffold molecolari selezionati in base a determinati criteri di bilanciamento del dataset. 
L'output `index` rappresenta gli indici delle molecole nel dataset originale che corrispondono ai scaffold selezionati."""


def scaffold_randomized_spliting_xiong(smiles_tasks_df, tasks:list,weights:list,random_seed=8):
    print('The dataset weights are', weights)
    print('generating scaffold......')
    scaffold_list = []
    all_scaffolds_dict = {}
    for index, smiles in enumerate(smiles_tasks_df['Smiles']):
        scaffold = generate_scaffold(smiles)
        scaffold_list.append(scaffold)
        if scaffold not in all_scaffolds_dict:
            all_scaffolds_dict[scaffold] = [index]
        else:
            all_scaffolds_dict[scaffold].append(index)
    #     smiles_tasks_df['scaffold'] = scaffold_list

    samples_size = int(len(all_scaffolds_dict.keys()) * 0.1) # 10% of the scaffold will be used
    test_scaffold, test_index = split(all_scaffolds_dict, smiles_tasks_df, tasks, weights, samples_size,
                                      random_seed=random_seed)
    training_scaffolds_dict = {x: all_scaffolds_dict[x] for x in all_scaffolds_dict.keys() if x not in test_scaffold}
    valid_scaffold, valid_index = split(training_scaffolds_dict, smiles_tasks_df, tasks, weights, samples_size,
                                        random_seed=random_seed)

    training_scaffolds_dict = {x: training_scaffolds_dict[x] for x in training_scaffolds_dict.keys() if
                               x not in valid_scaffold}
    train_index = []
    for ele in training_scaffolds_dict.values(): 
        train_index += ele  
    assert len(train_index) + len(valid_index) + len(test_index) == len(smiles_tasks_df)

    return train_index, valid_index, test_index


def generate_scaffold_hu(smiles, include_chirality=False):
    """
    Obtain Bemis-Murcko scaffold from smiles
    :param smiles:
    :param include_chirality:
    :return: smiles of scaffold
    """
    scaffold = MurckoScaffold.MurckoScaffoldSmiles(
        smiles=smiles, includeChirality=include_chirality)
    return scaffold

"""Questo file implementa una suddivisione basata sugli scaffold in modo deterministico. 
La funzione split seleziona scaffold molecolari in modo sistematico per creare suddivisioni bilanciate del dataset, a
ssicurando che le classi minoritarie siano rappresentate adeguatamente."""
def scaffold_split(dataset, smiles_list, task_idx=None, null_value=0,
                   frac_train=0.8, frac_valid=0.1, frac_test=0.1,
                   return_smiles=False):
    """
    Adapted from https://github.com/deepchem/deepchem/blob/master/deepchem/splits/splitters.py
    Split dataset by Bemis-Murcko scaffolds
    This function can also ignore examples containing null values for a
    selected task when splitting. Deterministic split
    :param dataset: pytorch geometric dataset obj
    :param smiles_list: list of smiles corresponding to the dataset obj
    :param task_idx: column idx of the data.y tensor. Will filter out
    examples with null value in specified task column of the data.y tensor
    prior to splitting. If None, then no filtering
    :param null_value: float that specifies null value in data.y to filter if
    task_idx is provided
    :param frac_train:
    :param frac_valid:
    :param frac_test:
    :param return_smiles:
    :return: train, valid, test slices of the input dataset obj. If
    return_smiles = True, also returns ([train_smiles_list],
    [valid_smiles_list], [test_smiles_list])
    """
    np.testing.assert_almost_equal(frac_train + frac_valid + frac_test, 1.0)

    if task_idx != None:
        # filter based on null values in task_idx
        # get task array
        y_task = np.array([data.y[task_idx].item() for data in dataset])
        # boolean array that correspond to non null values
        non_null = y_task != null_value
        smiles_list = list(compress(enumerate(smiles_list), non_null))
    else:
        non_null = np.ones(len(dataset)) == 1
        smiles_list = list(compress(enumerate(smiles_list), non_null))

    # create dict of the form {scaffold_i: [idx1, idx....]}
    all_scaffolds = {}
    for i, smiles in smiles_list:
        scaffold = generate_scaffold_hu(smiles, include_chirality=True)
        if scaffold not in all_scaffolds:
            all_scaffolds[scaffold] = [i]
        else:
            all_scaffolds[scaffold].append(i)

    # sort from largest to smallest sets
    all_scaffolds = {key: sorted(value) for key, value in all_scaffolds.items()}
    all_scaffold_sets = [
        scaffold_set for (scaffold, scaffold_set) in sorted(
            all_scaffolds.items(), key=lambda x: (len(x[1]), x[1][0]), reverse=True)
    ]

    # get train, valid test indices
    train_cutoff = frac_train * len(smiles_list)
    valid_cutoff = (frac_train + frac_valid) * len(smiles_list)
    train_idx, valid_idx, test_idx = [], [], []
    for scaffold_set in all_scaffold_sets:
        if len(train_idx) + len(scaffold_set) > train_cutoff:
            if len(train_idx) + len(valid_idx) + len(scaffold_set) > valid_cutoff:
                test_idx.extend(scaffold_set)
            else:
                valid_idx.extend(scaffold_set)
        else:
            train_idx.extend(scaffold_set)

    assert len(set(train_idx).intersection(set(valid_idx))) == 0
    assert len(set(test_idx).intersection(set(valid_idx))) == 0

    train_dataset = dataset[torch.tensor(train_idx)]
    valid_dataset = dataset[torch.tensor(valid_idx)]
    test_dataset = dataset[torch.tensor(test_idx)]

    if not return_smiles:
        return train_dataset, valid_dataset, test_dataset
    else:
        train_smiles = [smiles_list[i][1] for i in train_idx]
        valid_smiles = [smiles_list[i][1] for i in valid_idx]
        test_smiles = [smiles_list[i][1] for i in test_idx]
        return train_dataset, valid_dataset, test_dataset, (train_smiles,
                                                            valid_smiles,
                                                            test_smiles)
    

"""Questo file implementa una strategia di suddivisione completamente casuale basata sugli scaffold. 
La funzione split seleziona gli scaffold in modo completamente casuale, 
senza alcun tentativo di bilanciamento, ma piuttosto per valutare la robustezza del modello rispetto alle suddivisioni casuali."""
def random_scaffold_split_hu(dataset, smiles_list, task_idx=None, null_value=0,
                   frac_train=0.8, frac_valid=0.1, frac_test=0.1, seed=0):
    """
    Adapted from https://github.com/pfnet-research/chainer-chemistry/blob/master/chainer_chemistry/dataset/splitters/scaffold_splitter.py
    Split dataset by Bemis-Murcko scaffolds
    This function can also ignore examples containing null values for a
    selected task when splitting. Deterministic split
    :param dataset: pytorch geometric dataset obj
    :param smiles_list: list of smiles corresponding to the dataset obj
    :param task_idx: column idx of the data.y tensor. Will filter out
    examples with null value in specified task column of the data.y tensor
    prior to splitting. If None, then no filtering
    :param null_value: float that specifies null value in data.y to filter if
    task_idx is provided
    :param frac_train:
    :param frac_valid:
    :param frac_test:
    :param seed;
    :return: train, valid, test slices of the input dataset obj
    """

    np.testing.assert_almost_equal(frac_train + frac_valid + frac_test, 1.0)

    if task_idx != None:
        # filter based on null values in task_idx
        # get task array
        y_task = np.array([data.y[task_idx].item() for data in dataset])
        # boolean array that correspond to non null values
        non_null = y_task != null_value
        smiles_list = list(compress(enumerate(smiles_list), non_null))
    else:
        non_null = np.ones(len(dataset)) == 1
        smiles_list = list(compress(enumerate(smiles_list), non_null))

    rng = np.random.RandomState(seed)

    scaffolds = defaultdict(list)
    for ind, smiles in smiles_list:
        scaffold = generate_scaffold_hu(smiles, include_chirality=True)
        scaffolds[scaffold].append(ind)

    scaffold_sets = rng.permutation(list(scaffolds.values()))

    n_total_valid = int(np.floor(frac_valid * len(dataset)))
    n_total_test = int(np.floor(frac_test * len(dataset)))

    train_idx = []
    valid_idx = []
    test_idx = []

    for scaffold_set in scaffold_sets:
        if len(valid_idx) + len(scaffold_set) <= n_total_valid:
            valid_idx.extend(scaffold_set)
        elif len(test_idx) + len(scaffold_set) <= n_total_test:
            test_idx.extend(scaffold_set)
        else:
            train_idx.extend(scaffold_set)

    # train_dataset = dataset[torch.tensor(train_idx)]
    # valid_dataset = dataset[torch.tensor(valid_idx)]
    # test_dataset = dataset[torch.tensor(test_idx)]

    return torch.tensor(train_idx), torch.tensor(valid_idx), torch.tensor(test_idx)  # return Index

